{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d65928",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "#from lmf import db\n",
    "\n",
    "# Following two imports are required for non-sage environments (python). \n",
    "# Comment out if using sage.\n",
    "from sage.arith.misc import primes_first_n \n",
    "from sage.all import libgap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e413c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d : Degree of the Artin representations\n",
    "# target : One of 'Is_Even', 'Proj', or 'Image'\n",
    "# target_args : Additional arguments for target\n",
    "#               For Proj, use GAP id (id1, id2)\n",
    "#               For Image, use group name (3T2, 4T1, etc.)\n",
    "\n",
    "def get_data(d, target, target_args=None):\n",
    "  if target not in ['Is_Even', 'Proj', 'Image']:\n",
    "    raise ValueError(\"target must be one of 'Is_Even', 'Proj', or 'Image'\")\n",
    "  data = pl.read_csv(f\"artin_reps_d{d}_v2.csv\", schema_overrides={\"Conductor\": pl.Int128})\n",
    "  \n",
    "  columns_ = [f'a_{p:03d}' for p in primes_first_n(168)]\n",
    "  X = data.select(columns_).to_numpy()\n",
    "  \n",
    "  if target == 'Is_Even':\n",
    "    y = data.select(\"Is_Even\").to_numpy().ravel()\n",
    "  elif target == 'Proj':\n",
    "    id1, id2 = target_args\n",
    "    y = data.select((pl.col('GAP_1') == id1) & (pl.col('GAP_2') == id2)).to_numpy().ravel()\n",
    "  elif target == 'Image':\n",
    "    y = data.select(pl.col('Image') == target_args).to_numpy().ravel()\n",
    "\n",
    "  return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9896ba54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ap_onehot(x, use_existing_values=False):\n",
    "  # If use_existing_value is true, just use x.unique() as the values used in one-hot encoding\n",
    "  # If not, find max(abs(x)), and use values from -max, -max+2, -max+3, ..., max-3, max-2, max\n",
    "\n",
    "  if use_existing_values:\n",
    "    x_vals = np.sort(np.unique(x))\n",
    "  else:\n",
    "    max_val = max(abs(x).max(), abs(x).min())\n",
    "    x_vals = list(range(-max_val, max_val + 1))\n",
    "    if max_val != 2:\n",
    "      x_vals.pop(len(x_vals) - 2) # Remove max_val - 1\n",
    "      x_vals.pop(1)                # Remove - (max_val - 1)\n",
    "\n",
    "  x_one_hot = np.zeros((x.shape[0], len(primes_first_n(168)) * len(x_vals)), dtype=int)\n",
    "  for i, p in enumerate(primes_first_n(168)):\n",
    "    for j, v in enumerate(x_vals):\n",
    "      x_one_hot[:, i * len(x_vals) + j] = (x[:, i] == v).astype(int)\n",
    "  feature_names = [f\"a_{p}:{v}\" for p in primes_first_n(168) for v in x_vals]\n",
    "  return x_one_hot, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1121037",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6511a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_distribution(x, y, labels=None):\n",
    "  max_val = max(abs(x).max(), abs(x).min())\n",
    "  \n",
    "  fig, axes = plt.subplots(3, max_val - 1, figsize=((max_val - 1) * 4, 9))\n",
    "\n",
    "  # First row : 1, 2, ..., max_val - 2, max_val\n",
    "  # Second row : 0\n",
    "  # Third row : -1, -2, ..., -(max_val - 2), -max_val\n",
    "\n",
    "  x_pos = x[y == 1]\n",
    "  x_neg = x[y == 0]\n",
    "\n",
    "  zero_pos_count = (x_pos == 0).sum(axis=1)\n",
    "  zero_neg_count = (x_neg == 0).sum(axis=1)\n",
    "  if labels is not None:\n",
    "    axes[1, 0].hist(zero_pos_count, bins='auto', color='blue', alpha=0.5, density=True, label=labels[0])\n",
    "    axes[1, 0].hist(zero_neg_count, bins='auto', color='red', alpha=0.5, density=True, label=labels[1])\n",
    "  else:\n",
    "    axes[1, 0].hist(zero_pos_count, bins='auto', color='blue', alpha=0.5, density=True)\n",
    "    axes[1, 0].hist(zero_neg_count, bins='auto', color='red', alpha=0.5, density=True)\n",
    "  axes[1, 0].set_title('Count of a_p=0')\n",
    "\n",
    "  for i in range(1, max_val):\n",
    "    v = i if i != max_val - 1 else max_val\n",
    "\n",
    "    v_pos_count = (x_pos == v).sum(axis=1)\n",
    "    v_neg_count = (x_neg == v).sum(axis=1)\n",
    "    neg_v_pos_count = (x_pos == -v).sum(axis=1)\n",
    "    neg_v_neg_count = (x_neg == -v).sum(axis=1)\n",
    "    if labels is not None:\n",
    "      axes[0, i - 1].hist(v_pos_count, bins='auto', color='blue', alpha=0.5, density=True, label=labels[0])\n",
    "      axes[0, i - 1].hist(v_neg_count, bins='auto', color='red', alpha=0.5, density=True, label=labels[1])\n",
    "      axes[2, i - 1].hist(neg_v_pos_count, bins='auto', color='blue', alpha=0.5, density=True, label=labels[0])\n",
    "      axes[2, i - 1].hist(neg_v_neg_count, bins='auto', color='red', alpha=0.5, density=True, label=labels[1])\n",
    "    else:\n",
    "      axes[0, i - 1].hist(v_pos_count, bins='auto', color='blue', alpha=0.5, density=True)\n",
    "      axes[0, i - 1].hist(v_neg_count, bins='auto', color='red', alpha=0.5, density=True)\n",
    "      axes[2, i - 1].hist(neg_v_pos_count, bins='auto', color='blue', alpha=0.5, density=True)\n",
    "      axes[2, i - 1].hist(neg_v_neg_count, bins='auto', color='red', alpha=0.5, density=True)\n",
    "    \n",
    "    axes[0, i - 1].set_title(f'Count of a_p={v}')\n",
    "    axes[2, i - 1].set_title(f'Count of a_p={-v}')\n",
    "   \n",
    "  # Legend at the top of the whole figure\n",
    "  handles, labels = axes[1, 0].get_legend_handles_labels()\n",
    "  fig.legend(handles, labels, loc='upper center', ncol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0274e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = get_data(3, 'Image', target_args='4T5')\n",
    "x_one_hot, feature_names = ap_onehot(x, use_existing_values=False)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# See analyse_count.ipynb for existing labels\n",
    "class_names = ['Is 4T5', \"Isn't 4T5\"]\n",
    "\n",
    "draw_distribution(x, y, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a403b459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree with raw x, y\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Decision Tree Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489f6ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the tree\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(clf, filled=True, feature_names=[f'a_{p:03d}' for p in primes_first_n(168)], class_names=class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b6b1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression with raw x, y\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred_logreg = log_reg.predict(X_test)\n",
    "\n",
    "print(\"Logistic Regression Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_logreg))\n",
    "\n",
    "print(\"Logistic Regression Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_logreg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f883f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the coefficients, ordered by abs value\n",
    "coefficients = log_reg.coef_[0]\n",
    "coef_df = pl.DataFrame({\n",
    "    'Feature': [f'a_{p:03d}' for p in primes_first_n(168)],\n",
    "    'Coefficient': coefficients,\n",
    "    'Abs_Coefficient': np.abs(coefficients)\n",
    "}).sort('Abs_Coefficient', descending=True).select(['Feature', 'Coefficient'])\n",
    "\n",
    "print(coef_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd575bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree with one-hot encoded x, y\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_one_hot, y, test_size=0.3, random_state=42)\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "print(\"Decision Tree (One-Hot Encoded) Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62a5b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the tree\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(clf, filled=True, feature_names=feature_names, class_names=class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a7205c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression with one-hot encoded x\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(x_train, y_train)\n",
    "\n",
    "y_pred_logreg = log_reg.predict(x_test)\n",
    "print(\"Logistic Regression (One-Hot Encoded) Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_logreg))\n",
    "\n",
    "print(\"Logistic Regression (One-Hot Encoded) Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_logreg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ad7b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the coefficients, ordered by abs value\n",
    "\n",
    "coefficients = log_reg.coef_[0]\n",
    "coef_df = pl.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': coefficients,\n",
    "    'Abs_Coefficient': np.abs(coefficients)\n",
    "}).sort('Abs_Coefficient', descending=True).select(['Feature', 'Coefficient'])\n",
    "print(coef_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nt-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
